################## Library Settings ###########
library(rJava)
library(KoNLP)
library(tm)
library(ggplot2)
library(wordcloud)
library(stringr)
library(extrafont)
library(reshape2)
library(lsa)
library(topicmodels)
library(igraph)
KoNLP::useNIADic()

extrafont::loadfonts(device="win")
windowsFonts(HG=windowsFont("SeoulHangang M"))
windowsFonts(NS=windowsFont("SeoulNamsan M"))
################################################


## 0.File load
setwd("C:/data/roundtable")
tab1 <- read.csv("1st_raw_table.csv", header=T, stringsAsFactors=F)
sum1 <- read.csv("1st_raw_summary.csv", header=T, stringsAsFactors=F)

setwd("C:/data/roundtable/results2")

## 1.전처리(cleaning) 작업.
########### Problem2. KoNLP가 품사태깅/토큰화가 정교하지 않음(ex/ 무기력한 -> NC로 태깅됨)
###########  -> 1.사전에 단어추가해서 해결되는지? 2.다른 형태소 분석기 사용 고려해볼 것

#함수로 만들어서 lapply 사용한다.
clean.1 <- function(x) {
  #구두점 및 순자 제거함수.
  #필요시에 자주 등장하는 의미없는 단어 추가바람
  x <- gsub("[[:punct:]]","",x)
  x <- gsub("[[:digit:]]","",x)
}

tab1_clean <- apply(tab1[,3:4], 1:2, clean.1) #clean.1 함수의 적용.
tab1_clean <- data.frame(q1_t = c(tab1_clean[seq(1,60,4),1]),
                         q2_t = c(tab1_clean[seq(2,60,4),1]),
                         q3_t = c(tab1_clean[seq(3,60,4),1]),
                         q4_t = c(tab1_clean[seq(4,60,4),1]),
                         q1_o = c(tab1_clean[seq(1,60,4),2]),
                         q2_o = c(tab1_clean[seq(2,60,4),2]),
                         q3_o = c(tab1_clean[seq(3,60,4),2]),
                         q4_o = c(tab1_clean[seq(4,60,4),2]))

sum1_clean <- apply(sum1[,3:4], 1:2, clean.1) #clean.1 함수의 적용.
sum1_clean <- data.frame(q1_t = c(sum1_clean[seq(1,60,4),1]),
                         q2_t = c(sum1_clean[seq(2,60,4),1]),
                         q3_t = c(sum1_clean[seq(3,60,4),1]),
                         q4_t = c(sum1_clean[seq(4,60,4),1]),
                         q1_o = c(sum1_clean[seq(1,60,4),2]),
                         q2_o = c(sum1_clean[seq(2,60,4),2]),
                         q3_o = c(sum1_clean[seq(3,60,4),2]),
                         q4_o = c(sum1_clean[seq(4,60,4),2]))


# 1.2.SimplePos22으로 의미를 가지는 단어들 추출
# "+"를 기준으로 분리시키기 -> 형태소 단위로 쪼갠다.
#stringr로 /PV, /pA, /PX<-보조용언이라 안써도 될듯?, 
#/MM, /MA 이게 들어가는 것만 뽑는다. -> 용언, 수식언
tab1_list <- apply(tab1_clean, 1:2, KoNLP::SimplePos22)
sum1_list <- apply(sum1_clean, 1:2, KoNLP::SimplePos22)

tab1_word <- vector("list", 120)
for (i in 1:120) {
  temp <- lapply(tab1_list[[i]], function(x) strsplit(x, split="[:+]"))
  temp <- unlist(temp)
  temp <- strsplit(temp, " ")
  temp <- unlist(temp)
  names(temp) <- NULL
  tab1_word[[i]] <- temp
}

sum1_word <- vector("list", 120)
for (i in 1:120) {
  temp <- lapply(sum1_list[[i]], function(x) strsplit(x, split="[:+]"))
  temp <- unlist(temp)
  temp <- strsplit(temp, " ")
  temp <- unlist(temp)
  names(temp) <- NULL
  sum1_word[[i]] <- temp
}

# 1.3.의미있는 품사만 추출해서 정제.
words2df_tag <- function(wordvec) {
  pv <- grep("PV", wordvec, value=T)
  pa <- grep("PA", wordvec, value=T)
  mm <- grep("MM", wordvec, value=T)
  ma <- grep("MA", wordvec, value=T)
  nc <- grep("NC", wordvec, value=T)
  
  pv <- paste0(gsub("/PV", "", pv), "다")
  pv <- gsub("다다", "다", pv)
  pa <- paste0(gsub("/PA", "", pa), "다")
  pa <- gsub("다다", "다", pa)
  mm <- gsub("/MM", "", mm)
  ma <- gsub("/MA", "", ma)
  nc <- gsub("/NC", "", nc)
  
  n1<-length(pv); n2<-length(pa); n3<-length(mm); n4<-length(ma); n5<-length(nc)
  temp <- cbind(c(pv, pa, mm, ma, nc),
                c(rep("pv",n1), rep("pa",n2), rep("mm",n3), rep("ma",n4), rep("nc",n5)))
  temp <- as.data.frame(temp)
  names(temp) <- c("word", "tag")
  temp$word <- as.character(temp$word)
  if(any(nchar(temp$word)==1)) {
    temp <- temp[-which(nchar(temp$word)==1),] #한 글자 단어 제거. 필요에 따라서 특정 품사에만 제한.
  }
  return(temp)
}

tab1_freq <- lapply(tab1_word, words2df_tag)  #단어와 품사가 기록됨
sum1_freq <- lapply(sum1_word, words2df_tag)  #단어와 품사가 기록됨

tab1_td_list <- vector("list",120)
sum1_td_list <- vector("list",120)
for (i in 1:120) {tab1_td_list[[i]] <- as.data.frame(table(tab1_freq[[i]]$word))}  #단어와 빈도가 기록됨.
for (i in 1:120) {sum1_td_list[[i]] <- as.data.frame(table(sum1_freq[[i]]$word))}  #단어와 빈도가 기록됨.








#### table - 교사
td1 <- Reduce(function(x,y) merge(x=x, y=y, by="Var1", all=T), tab1_td_list[seq(1,60,4)])
td2 <- Reduce(function(x,y) merge(x=x, y=y, by="Var1", all=T), tab1_td_list[seq(2,60,4)])
td3 <- Reduce(function(x,y) merge(x=x, y=y, by="Var1", all=T), tab1_td_list[seq(3,60,4)])
td4 <- Reduce(function(x,y) merge(x=x, y=y, by="Var1", all=T), tab1_td_list[seq(4,60,4)])
#NA를 0으로.
td1[is.na(td1)]<-0
td2[is.na(td2)]<-0
td3[is.na(td3)]<-0
td4[is.na(td4)]<-0


#### table - 사정관
td5 <- Reduce(function(x,y) merge(x=x, y=y, by="Var1", all=T), tab1_td_list[seq(61,120,4)])
td6 <- Reduce(function(x,y) merge(x=x, y=y, by="Var1", all=T), tab1_td_list[seq(62,120,4)])
td7 <- Reduce(function(x,y) merge(x=x, y=y, by="Var1", all=T), tab1_td_list[seq(63,120,4)])
td8 <- Reduce(function(x,y) merge(x=x, y=y, by="Var1", all=T), tab1_td_list[seq(64,120,4)])
#NA를 0으로.
td5[is.na(td5)]<-0
td6[is.na(td6)]<-0
td7[is.na(td7)]<-0
td8[is.na(td8)]<-0

row.names(td1) <- 1:length(td1$Var1)
row.names(td2) <- 1:length(td2$Var1)
row.names(td3) <- 1:length(td3$Var1)
row.names(td4) <- 1:length(td4$Var1)
row.names(td5) <- 1:length(td5$Var1)
row.names(td6) <- 1:length(td6$Var1)
row.names(td7) <- 1:length(td7$Var1)
row.names(td8) <- 1:length(td8$Var1)

td1merge <- data.frame(term = td1$Var1, freq = rowSums(td1[,-1]))
td2merge <- data.frame(term = td2$Var1, freq = rowSums(td2[,-1]))
td3merge <- data.frame(term = td3$Var1, freq = rowSums(td3[,-1]))
td4merge <- data.frame(term = td4$Var1, freq = rowSums(td4[,-1]))
td5merge <- data.frame(term = td5$Var1, freq = rowSums(td5[,-1]))
td6merge <- data.frame(term = td6$Var1, freq = rowSums(td6[,-1]))
td7merge <- data.frame(term = td7$Var1, freq = rowSums(td7[,-1]))
td8merge <- data.frame(term = td8$Var1, freq = rowSums(td8[,-1]))

td <- Reduce(function(x,y) merge(x=x, y=y, by="term", all=T),
             list(td1merge,td2merge,td3merge,td4merge,td5merge,td6merge,td7merge,td8merge))
td[is.na(td)]<-0
names(td) <- c("term", "q1t","q2t","q3t","q4t","q1o","q2o","q3o","q4o")

## 의미없는 단어 제거1 - 빈도분석용도
meanless <- c(which(td$term=="되다"), which(td$term=="성장하"), 
              which(td$term=="아니다"), which(td$term=="어떤"),
              which(td$term=="또한"), which(td$term=="하다"),
              which(td$term=="가다"), which(td$term=="이런"),
              which(td$term=="너무"), which(td$term=="경우"))
td <- td[-meanless,]
write.csv(td, "td.csv")





dim(td)
td_red <- td[-which(rowSums(td[,-1])==1),]
dim(td_red)

idf_trim <- function(td) {
  mat <- td[,-1]
  names <- td[,1]

  idf = dim(mat)[2]/rowSums(mat>0)
  temp <- quantile(idf,seq(0,1,0.05))

  #trim
  if (temp[2]!=temp[3]) {
    mat <- mat[-which(idf<=temp[2]),]
    names <- names[-which(idf<=temp[2])]}   #너무 자주 나옴.
  td2 <- cbind.data.frame(names, mat)
  
  return(td2)
}

#td_red <- idf_trim(td_red)

### 1.빈도 살펴보기
a1 <- td_red[,c(1,2)][td_red[,2]>2,]; names(a1) <- c("term","freq")
a2 <- td_red[,c(1,3)][td_red[,3]>2,]; names(a2) <- c("term","freq")
a3 <- td_red[,c(1,4)][td_red[,4]>2,]; names(a3) <- c("term","freq")
a4 <- td_red[,c(1,5)][td_red[,5]>2,]; names(a4) <- c("term","freq")
a5 <- td_red[,c(1,6)][td_red[,6]>2,]; names(a5) <- c("term","freq")
a6 <- td_red[,c(1,7)][td_red[,7]>2,]; names(a6) <- c("term","freq")
a7 <- td_red[,c(1,8)][td_red[,8]>2,]; names(a7) <- c("term","freq")
a8 <- td_red[,c(1,9)][td_red[,9]>2,]; names(a8) <- c("term","freq")

filenames <- c("a1","a2","a3","a4","a5","a6","a7","a8")
filepng <- list(a1,a2,a3,a4,a5,a6,a7,a8)

for (i in 1:8) {
  p <- ggplot(filepng[[i]], aes(x=term, y=freq, fill=freq)) + geom_bar(stat="identity") +
      theme_bw() + xlab("출현단어") +ylab("단어빈도") +
      theme(axis.text.x = element_text(family="HG", size=7, angle=60, vjust=0.6, colour="black"),
            axis.title = element_text(family="HG")) +
      scale_fill_gradient(low="grey94", high="black") +
      guides(fill=FALSE)
  ggsave(paste0(filenames[i],".png"),p)
}

### 2.잠재의미분석
tdlist <- list(td1, td2, td3, td4, td5, td6, td7, td8)

lsafun <- function(tdobject,dim, tfidf=F, trim=F) {
  #pca = 설명분산 확인
  #lsa = tk,dk,sk 산출
  
  mat <- tdobject[,-1]
  
  #tf-idf
  if (tfidf==T) {
    idf = 15/rowSums(mat>0)
    mat <- mat * idf}
  
  temp3 <- quantile(idf,seq(0,1,0.05))

  #trim
  if (trim==T & temp3[2]!=temp3[3]) {
    mat <- mat[-which(idf<temp3[2]),]}   #너무 자주 나옴.
    
  temp1 <- summary(princomp(mat))
  temp2 <- lsa(mat,dim)

  return(list(pca=temp1, lsa=temp2, idf=temp3))
}

  
axisdist <- function(tdobject, length=20, trim=F, tfidf=F, dim=2) {
  ## 단어-점수 데이터 만들기
  # length = 상위 몇 개 단어?
  lsaresult <- lsafun(tdobject,5,trim=trim,tfidf=tfidf)$lsa

  label = tdobject[,1]
  axisinfo1 = order(abs(lsaresult$tk[,1]),decreasing = T)[1:length]
  axisinfo2 = order(abs(lsaresult$tk[,2]),decreasing = T)[1:length]
  axisinfo3 = order(abs(lsaresult$tk[,3]),decreasing = T)[1:length]
  axisinfo4 = order(abs(lsaresult$tk[,4]),decreasing = T)[1:length]
  axisinfo5 = order(abs(lsaresult$tk[,5]),decreasing = T)[1:length]
  
  Axis1_label <- as.character(label[axisinfo1])
  Axis2_label <- as.character(label[axisinfo2])
  Axis3_label <- as.character(label[axisinfo3])
  Axis4_label <- as.character(label[axisinfo4])
  Axis5_label <- as.character(label[axisinfo5])

  Axis1_score <- lsaresult$tk[axisinfo1,1]
  Axis2_score <- lsaresult$tk[axisinfo2,2]
  Axis3_score <- lsaresult$tk[axisinfo3,3]
  Axis4_score <- lsaresult$tk[axisinfo4,4]
  Axis5_score <- lsaresult$tk[axisinfo5,5]
  
  tlist <- list(Axis1_label,Axis2_label,Axis3_label,Axis4_label,Axis5_label)
  score <- list(abs(Axis1_score),abs(Axis2_score),Axis3_score,Axis4_score,Axis5_score)
  axis <- c(rep("Axis1",length),rep("Axis2",length),
            rep("Axis3",length),rep("Axis4",length),rep("Axis5",length))
  
  return(data.frame(term=unlist(tlist[1:dim]),
                    score=unlist(score[1:dim]),
                    axis=axis[1:(dim*length)]))
}


tdlist <- list(td1,td2,td3,td4,td5,td6,td7,td8)
blist <- vector("list",8)
for (i in 1:8) {
  tdn <- tdlist[[i]]
  meanless <- c(which(tdn$Var1=="되다"), which(tdn$Var1=="성장하"), 
                which(tdn$Var1=="아니다"), which(tdn$Var1=="어떤"),
                which(tdn$Var1=="또한"), which(tdn$Var1=="하다"),
                which(tdn$Var1=="가다"), which(tdn$Var1=="이런"),
                which(tdn$Var1=="너무"), which(tdn$Var1=="경우"),
                which(tdn$Var1=="년수업"), which(tdn$Var1=="그러다"),
                which(tdn$Var1=="생기다"), which(tdn$Var1=="모든"),
                which(tdn$Var1=="경우"), which(tdn$Var1=="결국"),
                which(tdn$Var1=="같다"), which(tdn$Var1=="것인가에"),
                which(tdn$Var1=="어떻다"), which(tdn$Var1=="부분"))
  tdn <- tdn[-meanless,]
  blist[[i]] <- axisdist(tdn,trim=F,tfidf=T, dim=2)}


filenames <- c("b1","b2","b3","b4","b5","b6","b7","b8")

for (i in 1:4) {
  p <- ggplot(blist[[i]], aes(x=term, y=score, fill=score)) + geom_bar(stat="identity") +
      facet_wrap(axis~., scale="free", nrow=2) + theme_bw() + xlab("출현단어") +ylab("단어빈도") +
      theme(axis.text.x = element_text(family="HG", size=13, angle=60, vjust=0.6, colour="black"),
            axis.title.x = element_text(family="NS", face="bold", size=12),
            axis.title.y = element_text(family="NS", face="bold", size=12))+
    scale_fill_gradient(low=brewer.pal(9,"Oranges")[2], high=brewer.pal(9,"Oranges")[8])
  ggsave(paste0(filenames[[i]],".png"))}

for (i in 5:8) {
  p <- ggplot(blist[[i]], aes(x=term, y=score, fill=score)) + geom_bar(stat="identity") +
      facet_wrap(axis~., scale="free", nrow=2) + theme_bw() + xlab("출현단어") +ylab("단어빈도") +
      theme(axis.text.x = element_text(family="HG", size=13, angle=60, vjust=0.6, colour="black"),
            axis.title.x = element_text(family="NS", face="bold", size=12),
            axis.title.y = element_text(family="NS", face="bold", size=12))+
    scale_fill_gradient(low=brewer.pal(9,"Blues")[2], high=brewer.pal(9,"Blues")[8])
  ggsave(paste0(filenames[[i]],".png"))}




### 2.1.NMF 적용?
### 3.잠재 디리크레 할당



