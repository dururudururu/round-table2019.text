################## Library Settings ###########
library(rJava)
library(KoNLP)
library(tm)
library(ggplot2)
library(wordcloud)
library(stringr)
library(extrafont)
library(reshape2)
library(lsa)
KoNLP::useNIADic()

extrafont::loadfonts(device="win")
windowsFonts(HG=windowsFont("08SeoulHangang M"))
################################################


## 0.File load
setwd("D:/round-table")
dat <- read.csv("1st_raw.csv", header=T, stringsAsFactors = F)


## 1.전처리(cleaning) 작업.

################################ Probs in process1 ###################################
#Problem1. 현재 kospacing이 작동하지 않음 -> 포매팅 작업때 띄어쓰기 및 오탈자 검정하기
#Problem2. KoNLP가 품사태깅/토큰화가 정교하지 않음(ex/ 무기력한 -> NC로 태깅됨)
# -> 1.사전에 단어추가해서 해결되는지? 2.다른 형태소 분석기 사용 고려해볼 것
######################################################################################

#함수로 만들어서 lapply 사용한다.
clean.1 <- function(x) {
  #구두점 및 순자 제거함수.
  #필요시에 자주 등장하는 의미없는 단어 추가바람
  x <- gsub("[[:punct:]]","",x)
  x <- gsub("[[:digit:]]","",x)
}

text_df <- apply(dat[,3:4], 1:2, clean.1) #clean.1 함수의 적용.


# 1.2.SimplePos22으로 의미를 가지는 단어들 추출
# "+"를 기준으로 분리시키기 -> 형태소 단위로 쪼갠다.
#stringr로 /PV, /pA, /PX<-보조용언이라 안써도 될듯?, 
#/MM, /MA 이게 들어가는 것만 뽑는다. -> 용언, 수식언
text_list <- apply(text_df, 1:2, KoNLP::SimplePos22)
text_word <- vector("list", 120)
for (i in 1:120) {
  temp <- lapply(text_list[[i]], function(x) strsplit(x, split="[:+]"))
  temp <- unlist(temp)
  temp <- strsplit(temp, " ")
  temp <- unlist(temp)
  names(temp) <- NULL
  text_word[[i]] <- temp
}

# 1.3.의미있는 품사만 추출해서 정제.
words2df_tag <- function(wordvec) {
  pv <- grep("PV", wordvec, value=T)
  pa <- grep("PA", wordvec, value=T)
  mm <- grep("MM", wordvec, value=T)
  ma <- grep("MA", wordvec, value=T)
  nc <- grep("NC", wordvec, value=T)
  
  pv <- paste0(gsub("/PV", "", pv), "다")
  pv <- gsub("다다", "다", pv)
  pa <- paste0(gsub("/PA", "", pa), "다")
  pa <- gsub("다다", "다", pa)
  mm <- gsub("/MM", "", mm)
  ma <- gsub("/MA", "", ma)
  nc <- gsub("/NC", "", nc)
  
  n1<-length(pv); n2<-length(pa); n3<-length(mm); n4<-length(ma); n5<-length(nc)
  temp <- cbind(c(pv, pa, mm, ma, nc),
                c(rep("pv",n1), rep("pa",n2), rep("mm",n3), rep("ma",n4), rep("nc",n5)))
  temp <- as.data.frame(temp)
  names(temp) <- c("word", "tag")
  temp$word <- as.character(temp$word)
  temp <- temp[-which(nchar(temp$word)<2),] #한 글자 단어 제거. 필요에 따라서 특정 품사에만 제한.
  return(temp)
}

word_freq <- lapply(text_word, words2df_tag)  #단어와 품사가 기록됨.

td_list <- vector("list",120)
for (i in 1:120) {td_list[[i]] <- as.data.frame(table(word_freq[[i]]$word))}  #단어와 빈도가 기록됨.

# td1은 교사, td2는 사정관의 Term-Document matrix(1행은 단어벡터)
td1 <- Reduce(function(x,y) merge(x=x, y=y, by="Var1", all=T), td_list[c(1:34,36:60)])  #한 글자 단어를 삭제하면, 35번이 빈 리스트임.
td1[is.na(td1)]<-0
names(td1) <- c("term", 1:59)

td2 <- Reduce(function(x,y) merge(x=x, y=y, by="Var1", all=T), td_list[61:120])
td2[is.na(td2)]<-0
names(td2) <- c("term", 1:60)



# 2.전체 빈도 살펴보기
total1 <- rowSums(td1[,-1])
top1 <- data.frame(term=td1$term[total1>=10], freq=total1[total1>=10])
total2 <- rowSums(td2[,-1])
top2 <- data.frame(term=td2$term[total2>=10], freq=total2[total2>=10])

png("freq1.png",height=3000, width=3000, res=500)
ggplot(top1, aes(x=freq, y=reorder(term, freq))) + 
  geom_point(size=1) + xlab("단어빈도(교사)") + ylab("출현단어") +
  theme_bw() + theme(axis.title=element_text(family='HG'),
                     axis.text=element_text(family='HG'))
dev.off()

png("freq2.png",height=3000, width=3000, res=500)
ggplot(top2, aes(x=freq, y=reorder(term, freq))) + 
  geom_point(size=1) + xlab("단어빈도(사정관)") + ylab("출현단어") +
  theme_bw() + theme(axis.title=element_text(family='HG'),
                     axis.text=element_text(family='HG'))
dev.off()

tab <- data.frame(rbind(top1,top2), c(rep(1,45), rep(2,35)))
png("wc1.png",height=1600, width=1600, res=500)
## IDEA: 공통단어를 교집합으로 묶어주는 communality cloud
##       두 집단을 비교 가능한 comparison cloud
wordcloud(words=tab$term, freq=log(tab$freq, base=5), colors=ifelse(tab[,3]==1,"red","blue"), max.words = 100,
          scale=c(0.1,1.5),rot.per=0, random.order = F, family='HG')
dev.off()



# 3.잠재의미분석
## 동일한데 표현형이 다른 단어들을 더 정제해야 함 -> 띄어쓰기 교정하고 형태소분석 재적용 바람.
## 무엇을 문서로 삼고, 매핑, 비교의 목적은 무엇인가? 60문서(테이블x주제), 15문서(테이블), 4문서(주제) x 2문서(교사/사정관)
td<-Reduce(function(x,y) merge(x=x,y=y,by="Var1", all=T), td_list[-35])
td[is.na(td)]<-0
LSA <-lsa(td[,-1], dim=2)
P<-LSA$dk %*% diag(LSA$sk)
gdf <- data.frame(P)
sub <- as.factor(c(rep(1,59),rep(2,60)))
gdf <- data.frame(sub,P)
ggplot(gdf)+aes(x=X1, y=X2, label=1:119)+geom_point(color=sub)
